{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3934aff3",
   "metadata": {},
   "source": [
    "# Data Splitting\n",
    "\n",
    "This notebooks take the output dataset from `scripts/dataset` and splits it into train/val/test, standardizing the features along the way using different mean and std for every user; and ordinally encoding the static features (for embedding lookup later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d990ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "324a645b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\n",
    "    \"./endomondoHR_proper_interpolated.parquet\",\n",
    "    engine=\"pyarrow\",\n",
    "    dtype_backend=\"pyarrow\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63b73b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by first timestamp by user\n",
    "df[\"first_timestamp\"] = df[\"timestamp\"].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9243e21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train/va/test\n",
    "# 80/10/10\n",
    "\n",
    "# We have to do proportional temporal split per user.\n",
    "\n",
    "# 1. Ensure data is sorted by User and Time (CRITICAL)\n",
    "df = df.sort_values(by=[\"first_timestamp\", \"userId\"])\n",
    "\n",
    "# 2. Calculate the position of each row relative to its user group\n",
    "user_id_groupby = df.groupby(\"userId\")\n",
    "\n",
    "# cumcount starts at 0, so row 1 is index 0\n",
    "df[\"user_row\"] = user_id_groupby.cumcount() + 1\n",
    "df[\"user_rows_total\"] = user_id_groupby[\"userId\"].transform(\"count\")\n",
    "\n",
    "df[\"user_rows_val_start\"] = (df[\"user_rows_total\"] * 0.8).astype(int)\n",
    "df[\"user_rows_test_start\"] = (df[\"user_rows_total\"] * 0.9).astype(int)\n",
    "\n",
    "train_df = df[df[\"user_row\"] < df[\"user_rows_val_start\"]].copy()\n",
    "val_df = df[\n",
    "    (df[\"user_row\"] >= df[\"user_rows_val_start\"])\n",
    "    & (df[\"user_row\"] < df[\"user_rows_test_start\"])\n",
    "].copy()\n",
    "test_df = df[df[\"user_row\"] >= df[\"user_rows_test_start\"]].copy()\n",
    "\n",
    "train_df.drop(\n",
    "    columns=[\n",
    "        \"first_timestamp\",\n",
    "        \"user_row\",\n",
    "        \"user_rows_total\",\n",
    "        \"user_rows_val_start\",\n",
    "        \"user_rows_test_start\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "val_df.drop(\n",
    "    columns=[\n",
    "        \"first_timestamp\",\n",
    "        \"user_row\",\n",
    "        \"user_rows_total\",\n",
    "        \"user_rows_val_start\",\n",
    "        \"user_rows_test_start\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "test_df.drop(\n",
    "    columns=[\n",
    "        \"first_timestamp\",\n",
    "        \"user_row\",\n",
    "        \"user_rows_total\",\n",
    "        \"user_rows_val_start\",\n",
    "        \"user_rows_test_start\",\n",
    "    ],\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85eac7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 887/887 [00:45<00:00, 19.38it/s]\n"
     ]
    }
   ],
   "source": [
    "# normalize them - learn the means and stds from train only - apply to val and test\n",
    "\n",
    "user_stats = {}\n",
    "columns_to_standardize = [\n",
    "    \"time_elapsed\",\n",
    "    \"heart_rate\",\n",
    "    \"altitude\",\n",
    "    \"derived_speed\",\n",
    "    \"derived_distance\",\n",
    "]\n",
    "for user_id, group in tqdm(train_df.groupby(\"userId\")):\n",
    "    user_stats[user_id] = {}\n",
    "\n",
    "    for col in columns_to_standardize:\n",
    "        user_stats[user_id][col] = []\n",
    "\n",
    "        # Concatenate all vectors for this user into one giant 1D array\n",
    "        all_values = np.concatenate(group[col].values)\n",
    "\n",
    "        user_stats[user_id][col].append(np.mean(all_values))\n",
    "        user_stats[user_id][col].append(np.std(all_values) + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a852182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Standardizing columns...: 100%|██████████| 5/5 [00:04<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "for col in tqdm(columns_to_standardize, desc=\"Standardizing columns...\"):\n",
    "    new_col = f\"{col}_standardized\"\n",
    "\n",
    "    train_df[new_col] = train_df.apply(\n",
    "        lambda row: (row[col] - user_stats[row[\"userId\"]][col][0])\n",
    "        / user_stats[row[\"userId\"]][col][1],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    val_df[new_col] = val_df.apply(\n",
    "        lambda row: (row[col] - user_stats[row[\"userId\"]][col][0])\n",
    "        / user_stats[row[\"userId\"]][col][1],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    test_df[new_col] = test_df.apply(\n",
    "        lambda row: (row[col] - user_stats[row[\"userId\"]][col][0])\n",
    "        / user_stats[row[\"userId\"]][col][1],\n",
    "        axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8a31485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinally encode static features\n",
    "user_mapping = {user_id: idx for idx, user_id in enumerate(train_df[\"userId\"].unique())}\n",
    "\n",
    "sport_mapping = {sport: idx for idx, sport in enumerate(train_df[\"sport\"].unique())}\n",
    "\n",
    "gender_mapping = {gender: idx for idx, gender in enumerate(train_df[\"gender\"].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6711c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"user_idx\"] = train_df[\"userId\"].map(user_mapping)\n",
    "train_df[\"sport_idx\"] = train_df[\"sport\"].map(sport_mapping)\n",
    "train_df[\"gender_idx\"] = train_df[\"gender\"].map(gender_mapping)\n",
    "\n",
    "val_df[\"user_idx\"] = val_df[\"userId\"].map(user_mapping)\n",
    "val_df[\"sport_idx\"] = val_df[\"sport\"].map(sport_mapping)\n",
    "val_df[\"gender_idx\"] = val_df[\"gender\"].map(gender_mapping)\n",
    "\n",
    "test_df[\"user_idx\"] = test_df[\"userId\"].map(user_mapping)\n",
    "test_df[\"sport_idx\"] = test_df[\"sport\"].map(sport_mapping)\n",
    "test_df[\"gender_idx\"] = test_df[\"gender\"].map(gender_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5da9b474",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = pa.schema(\n",
    "    [\n",
    "        pa.field(\"id\", pa.int64()),\n",
    "        pa.field(\"userId\", pa.int64()),\n",
    "        pa.field(\"sport\", pa.string()),\n",
    "        pa.field(\"gender\", pa.string()),\n",
    "        pa.field(\"timestamp\", pa.list_(pa.int64())),\n",
    "        pa.field(\"derived_speed\", pa.list_(pa.float64())),\n",
    "        pa.field(\"heart_rate\", pa.list_(pa.float64())),\n",
    "        pa.field(\"longitude\", pa.list_(pa.float64())),\n",
    "        pa.field(\"latitude\", pa.list_(pa.float64())),\n",
    "        pa.field(\"derived_distance\", pa.list_(pa.float64())),\n",
    "        pa.field(\"time_elapsed\", pa.list_(pa.int64())),\n",
    "        pa.field(\"altitude\", pa.list_(pa.float64())),\n",
    "        pa.field(\"time_elapsed_standardized\", pa.list_(pa.float64())),\n",
    "        pa.field(\"heart_rate_standardized\", pa.list_(pa.float64())),\n",
    "        pa.field(\"altitude_standardized\", pa.list_(pa.float64())),\n",
    "        pa.field(\"derived_speed_standardized\", pa.list_(pa.float64())),\n",
    "        pa.field(\"derived_distance_standardized\", pa.list_(pa.float64())),\n",
    "        pa.field(\"user_idx\", pa.int64()),\n",
    "        pa.field(\"sport_idx\", pa.int64()),\n",
    "        pa.field(\"gender_idx\", pa.int64()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_df.to_parquet(\n",
    "    \"train.parquet\",\n",
    "    schema=schema,\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "\n",
    "\n",
    "val_df.to_parquet(\n",
    "    \"val.parquet\",\n",
    "    schema=schema,\n",
    "    engine=\"pyarrow\",\n",
    ")\n",
    "\n",
    "\n",
    "test_df.to_parquet(\n",
    "    \"test.parquet\",\n",
    "    schema=schema,\n",
    "    engine=\"pyarrow\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1748d87d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-preparation-2026-group-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
