{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using src: C:\\Users\\20221529\\OneDrive - TU Eindhoven\\Desktop\\Data Preperation\\BigAssignment\\data-preparation-2026-group-project\\src\n"
          ]
        }
      ],
      "source": [
        "# ---- Setup & imports (biking 3-variant inference test) ----\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "import plotly.io as pio\n",
        "import torch\n",
        "\n",
        "# Ensure \"src/\" is on sys.path so \"import project\" works (src-layout repo).\n",
        "def add_src_to_path():\n",
        "    p = Path.cwd().resolve()\n",
        "    for parent in [p] + list(p.parents):\n",
        "        if (parent / \"src\" / \"project\").exists():\n",
        "            src = parent / \"src\"\n",
        "            if str(src) not in sys.path:\n",
        "                sys.path.insert(0, str(src))\n",
        "            print(\"Using src:\", src)\n",
        "            return\n",
        "    raise RuntimeError(\"Could not locate src/project. Start Jupyter somewhere inside the repo tree.\")\n",
        "\n",
        "add_src_to_path()\n",
        "\n",
        "from project.utils.dataset import derive_features\n",
        "from project.utils.modeling import predict_model\n",
        "from project.utils.visuals import plot_target_vs_predicted_heartrate\n",
        "\n",
        "pio.renderers.default = \"svg\"\n",
        "\n",
        "def read_parquet_pylist(path: Path) -> pd.DataFrame:\n",
        "    \"\"\"Robust reader for list-columns without pandas/pyarrow extension dtype issues.\"\"\"\n",
        "    table = pq.read_table(path, use_pandas_metadata=False)\n",
        "    return pd.DataFrame({c: table[c].to_pylist() for c in table.column_names})\n",
        "\n",
        "def flatten_true_pred(df: pd.DataFrame, true_col=\"heart_rate\", pred_col=\"predicted_heart_rate\", skip_first=True):\n",
        "    yt_all, yp_all = [], []\n",
        "    for _, row in df.iterrows():\n",
        "        yt = np.asarray(row[true_col], dtype=float)\n",
        "        yp = np.asarray(row[pred_col], dtype=float)\n",
        "        n = min(len(yt), len(yp))\n",
        "        if n <= 1:\n",
        "            continue\n",
        "        start = 1 if skip_first else 0\n",
        "        yt_all.append(yt[start:n])\n",
        "        yp_all.append(yp[start:n])\n",
        "    if not yt_all:\n",
        "        return np.asarray([]), np.asarray([])\n",
        "    return np.concatenate(yt_all), np.concatenate(yp_all)\n",
        "\n",
        "def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray):\n",
        "    if y_true.size == 0:\n",
        "        return float(\"nan\"), float(\"nan\"), 0\n",
        "    mae = float(np.mean(np.abs(y_true - y_pred)))\n",
        "    rmse = float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
        "    return mae, rmse, int(y_true.size)\n",
        "\n",
        "def apply_user_standard_scaler_to_sequences(df, scaler, id_col=\"userId\"):\n",
        "    \"\"\"\n",
        "    Creates *_standardized columns for list-valued signals using scaler.user_stats.\n",
        "    Supports common layouts:\n",
        "      A) user_stats[col][user] = (mean, std) or {\"mean\":..,\"std\":..}\n",
        "      B) user_stats[user][col] = (mean, std) or {\"mean\":..,\"std\":..}\n",
        "    \"\"\"\n",
        "    if not hasattr(scaler, \"user_stats\"):\n",
        "        raise TypeError(\"Expected scaler.user_stats to exist.\")\n",
        "\n",
        "    df = df.copy()\n",
        "    raw_cols = [\"time_elapsed\", \"altitude\", \"derived_speed\", \"derived_distance\", \"heart_rate\"]\n",
        "\n",
        "    us = scaler.user_stats\n",
        "\n",
        "    def get_mu_sigma(user, col):\n",
        "        # Layout A: user_stats[col][user]\n",
        "        if isinstance(us, dict) and col in us and isinstance(us[col], dict) and user in us[col]:\n",
        "            v = us[col][user]\n",
        "        # Layout B: user_stats[user][col]\n",
        "        elif isinstance(us, dict) and user in us and isinstance(us[user], dict) and col in us[user]:\n",
        "            v = us[user][col]\n",
        "        else:\n",
        "            # If user/col missing, fall back to neutral scaling\n",
        "            return 0.0, 1.0\n",
        "\n",
        "        # v can be (mean,std) or {\"mean\":..,\"std\":..}\n",
        "        if isinstance(v, dict):\n",
        "            mu = float(v.get(\"mean\", 0.0))\n",
        "            sig = float(v.get(\"std\", 1.0))\n",
        "        elif isinstance(v, (tuple, list)) and len(v) == 2:\n",
        "            mu = float(v[0])\n",
        "            sig = float(v[1])\n",
        "        else:\n",
        "            # Unknown structure\n",
        "            return 0.0, 1.0\n",
        "\n",
        "        if not np.isfinite(sig) or sig <= 1e-12:\n",
        "            sig = 1.0\n",
        "        if not np.isfinite(mu):\n",
        "            mu = 0.0\n",
        "        return mu, sig\n",
        "\n",
        "    users = df[id_col].tolist()\n",
        "\n",
        "    for col in raw_cols:\n",
        "        out = []\n",
        "        for u, seq in zip(users, df[col].tolist()):\n",
        "            mu, sig = get_mu_sigma(u, col)\n",
        "            arr = np.asarray(seq, dtype=float)\n",
        "            out.append(((arr - mu) / sig).tolist())\n",
        "        df[f\"{col}_standardized\"] = out\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0350f36",
      "metadata": {},
      "source": [
        "for biking dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BASE: C:\\Users\\20221529\\OneDrive - TU Eindhoven\\Desktop\\Data Preperation\\BigAssignment\\data-preparation-2026-group-project\\src\\project\\temp\\biking\n",
            "baseline -> C:\\Users\\20221529\\OneDrive - TU Eindhoven\\Desktop\\Data Preperation\\BigAssignment\\data-preparation-2026-group-project\\src\\project\\temp\\biking\\biking_test_raw.parquet\n",
            "corrupted -> C:\\Users\\20221529\\OneDrive - TU Eindhoven\\Desktop\\Data Preperation\\BigAssignment\\data-preparation-2026-group-project\\src\\project\\temp\\biking\\biking_test_raw_corrupted.parquet\n",
            "cleaned -> C:\\Users\\20221529\\OneDrive - TU Eindhoven\\Desktop\\Data Preperation\\BigAssignment\\data-preparation-2026-group-project\\src\\project\\temp\\biking\\biking_test_raw_corrupted_cleaned.parquet\n"
          ]
        }
      ],
      "source": [
        "# ---- Paths (biking artifacts) ----\n",
        "# Notebook CWD is typically: .../src/project/notebooks\n",
        "# We want: .../src/project/temp/biking\n",
        "BASE = Path.cwd().resolve().parent / \"temp\" / \"biking\"\n",
        "\n",
        "paths = {\n",
        "    \"baseline\":  BASE / \"biking_test_raw.parquet\",\n",
        "    \"corrupted\": BASE / \"biking_test_raw_corrupted.parquet\",\n",
        "    \"cleaned\":   BASE / \"biking_test_raw_corrupted_cleaned.parquet\",\n",
        "}\n",
        "\n",
        "STANDARD_SCALER_PATH = BASE / \"biking_user_standard_scaler.pkl\"\n",
        "STATIC_ENCODER_PATH  = BASE / \"biking_static_ordinal_encoder.pkl\"\n",
        "MODEL_PATH           = BASE / \"biking_fitrec_model.pt\"\n",
        "\n",
        "# THESE SHOULD MATCH THE ONES USED DURING TRAINING\n",
        "dataset_arguments = {\n",
        "    \"numerical_columns\": [\n",
        "        \"time_elapsed_standardized\",\n",
        "        \"altitude_standardized\",\n",
        "        \"derived_speed_standardized\",\n",
        "        \"derived_distance_standardized\",\n",
        "    ],\n",
        "    \"categorical_columns\": [\"userId_idx\", \"sport_idx\", \"gender_idx\"],\n",
        "    \"heartrate_input_column\": \"heart_rate_standardized\",\n",
        "    \"heartrate_output_column\": \"heart_rate\",\n",
        "    \"workout_id_column\": \"id\",\n",
        "    \"use_heartrate_input\": True,\n",
        "}\n",
        "\n",
        "print(\"BASE:\", BASE)\n",
        "for k, v in paths.items():\n",
        "    print(k, \"->\", v)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3addb109",
      "metadata": {},
      "source": [
        "for running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4146c27b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Paths (running artifacts) ----\n",
        "# Notebook CWD is typically: .../src/project/notebooks\n",
        "# We want: .../src/project/temp/running\n",
        "BASE = Path.cwd().resolve().parent / \"temp\" / \"running\"\n",
        "\n",
        "paths = {\n",
        "    \"baseline\":  BASE / \"running_test_raw.parquet\",\n",
        "    \"corrupted\": BASE / \"running_test_raw_corrupted.parquet\",\n",
        "    \"cleaned\":   BASE / \"running_test_raw_corrupted_cleaned.parquet\",\n",
        "}\n",
        "\n",
        "STANDARD_SCALER_PATH = BASE / \"running_user_standard_scaler.pkl\"\n",
        "STATIC_ENCODER_PATH  = BASE / \"running_static_ordinal_encoder.pkl\"\n",
        "MODEL_PATH           = BASE / \"running_fitrec_model.pt\"\n",
        "\n",
        "# THESE SHOULD MATCH THE ONES USED DURING TRAINING\n",
        "dataset_arguments = {\n",
        "    \"numerical_columns\": [\n",
        "        \"time_elapsed_standardized\",\n",
        "        \"altitude_standardized\",\n",
        "        \"derived_speed_standardized\",\n",
        "        \"derived_distance_standardized\",\n",
        "    ],\n",
        "    \"categorical_columns\": [\"userId_idx\", \"sport_idx\", \"gender_idx\"],\n",
        "    \"heartrate_input_column\": \"heart_rate_standardized\",\n",
        "    \"heartrate_output_column\": \"heart_rate\",\n",
        "    \"workout_id_column\": \"id\",\n",
        "    \"use_heartrate_input\": True,\n",
        "}\n",
        "\n",
        "print(\"BASE:\", BASE)\n",
        "for k, v in paths.items():\n",
        "    print(k, \"->\", v)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c1b85cb",
      "metadata": {},
      "source": [
        "for walking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a290ff1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Paths (running artifacts) ----\n",
        "# Notebook CWD is typically: .../src/project/notebooks\n",
        "# We want: .../src/project/temp/walking\n",
        "BASE = Path.cwd().resolve().parent / \"temp\" / \"walking\"\n",
        "\n",
        "paths = {\n",
        "    \"baseline\":  BASE / \"walking_test_raw.parquet\",\n",
        "    \"corrupted\": BASE / \"walking_test_raw_corrupted.parquet\",\n",
        "    \"cleaned\":   BASE / \"walking_test_raw_corrupted_cleaned.parquet\",\n",
        "}\n",
        "\n",
        "STANDARD_SCALER_PATH = BASE / \"walking_user_standard_scaler.pkl\"\n",
        "STATIC_ENCODER_PATH  = BASE / \"walking_static_ordinal_encoder.pkl\"\n",
        "MODEL_PATH           = BASE / \"walking_fitrec_model.pt\"\n",
        "\n",
        "# THESE SHOULD MATCH THE ONES USED DURING TRAINING\n",
        "dataset_arguments = {\n",
        "    \"numerical_columns\": [\n",
        "        \"time_elapsed_standardized\",\n",
        "        \"altitude_standardized\",\n",
        "        \"derived_speed_standardized\",\n",
        "        \"derived_distance_standardized\",\n",
        "    ],\n",
        "    \"categorical_columns\": [\"userId_idx\", \"sport_idx\", \"gender_idx\"],\n",
        "    \"heartrate_input_column\": \"heart_rate_standardized\",\n",
        "    \"heartrate_output_column\": \"heart_rate\",\n",
        "    \"workout_id_column\": \"id\",\n",
        "    \"use_heartrate_input\": True,\n",
        "}\n",
        "\n",
        "print(\"BASE:\", BASE)\n",
        "for k, v in paths.items():\n",
        "    print(k, \"->\", v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All required files found.\n"
          ]
        }
      ],
      "source": [
        "# ---- Existence checks ----\n",
        "assert BASE.exists(), f\"BASE does not exist: {BASE}\"\n",
        "assert STANDARD_SCALER_PATH.exists(), f\"Missing scaler: {STANDARD_SCALER_PATH}\"\n",
        "assert STATIC_ENCODER_PATH.exists(), f\"Missing encoder: {STATIC_ENCODER_PATH}\"\n",
        "assert MODEL_PATH.exists(), f\"Missing model: {MODEL_PATH}\"\n",
        "\n",
        "for name, p in paths.items():\n",
        "    assert p.exists(), f\"Missing {name} parquet: {p}\"\n",
        "\n",
        "print(\"All required files found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded scaler: <class 'project.utils.dataset.preprocessing.UserStandardScaler'>\n",
            "Loaded encoder: <class 'project.utils.dataset.preprocessing.StaticFeatureOrdinalEncoder'>\n",
            "Loaded model: <class 'project.utils.modeling.model.FitRecModel'>\n"
          ]
        }
      ],
      "source": [
        "# ---- Load artifacts ----\n",
        "standard_scaler = pickle.load(open(STANDARD_SCALER_PATH, \"rb\"))\n",
        "static_encoder  = pickle.load(open(STATIC_ENCODER_PATH, \"rb\"))\n",
        "\n",
        "model = torch.load(MODEL_PATH, weights_only=False, map_location=\"cpu\")\n",
        "model.eval()\n",
        "\n",
        "print(\"Loaded scaler:\", type(standard_scaler))\n",
        "print(\"Loaded encoder:\", type(static_encoder))\n",
        "print(\"Loaded model:\", type(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 300/300 [01:18<00:00,  3.81it/s]\n",
            "100%|██████████| 300/300 [01:12<00:00,  4.14it/s]\n",
            "100%|██████████| 300/300 [01:12<00:00,  4.12it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>variant</th>\n",
              "      <th>MAE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>n_points</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>baseline</td>\n",
              "      <td>1.940390</td>\n",
              "      <td>3.049630</td>\n",
              "      <td>3077906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cleaned</td>\n",
              "      <td>5.324597</td>\n",
              "      <td>16.777649</td>\n",
              "      <td>3077906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>corrupted</td>\n",
              "      <td>9.899215</td>\n",
              "      <td>33.370995</td>\n",
              "      <td>3077906</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     variant       MAE       RMSE  n_points\n",
              "0   baseline  1.940390   3.049630   3077906\n",
              "2    cleaned  5.324597  16.777649   3077906\n",
              "1  corrupted  9.899215  33.370995   3077906"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ---- Run inference for each variant and compute metrics ----\n",
        "def run_variant(name: str, parquet_path: Path):\n",
        "    df = read_parquet_pylist(parquet_path)\n",
        "\n",
        "    # Derive extra features (distance/speed etc.)\n",
        "    df = derive_features(df)\n",
        "\n",
        "    # Standardize sequences (robust to list-columns)\n",
        "    df = apply_user_standard_scaler_to_sequences(df, standard_scaler, id_col=\"userId\")\n",
        "\n",
        "    # Encode static categorical features -> *_idx\n",
        "    df = static_encoder.transform(df)\n",
        "\n",
        "    # Contract check before inference\n",
        "    required = set(\n",
        "        dataset_arguments[\"numerical_columns\"]\n",
        "        + dataset_arguments[\"categorical_columns\"]\n",
        "        + [\n",
        "            dataset_arguments[\"heartrate_input_column\"],\n",
        "            dataset_arguments[\"heartrate_output_column\"],\n",
        "            dataset_arguments[\"workout_id_column\"],\n",
        "        ]\n",
        "    )\n",
        "    missing = required.difference(df.columns)\n",
        "    assert not missing, f\"[{name}] Missing required columns after preprocessing: {missing}\"\n",
        "\n",
        "    # Predict\n",
        "    y_preds = predict_model(model, df, dataset_args=dataset_arguments, n_workers=8)\n",
        "    df[\"predicted_heart_rate\"] = list(y_preds)\n",
        "\n",
        "    # Score (skip first step due to autoregressive HR input)\n",
        "    y_true, y_pred = flatten_true_pred(df, skip_first=True)\n",
        "    mae, rmse, n_points = compute_metrics(y_true, y_pred)\n",
        "\n",
        "    return df, {\"variant\": name, \"MAE\": mae, \"RMSE\": rmse, \"n_points\": n_points}\n",
        "\n",
        "dfs = {}\n",
        "rows = []\n",
        "for name, p in paths.items():\n",
        "    df_out, metrics = run_variant(name, p)\n",
        "    dfs[name] = df_out\n",
        "    rows.append(metrics)\n",
        "\n",
        "results_df = pd.DataFrame(rows).sort_values(\"variant\")\n",
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'dfs' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_36032/1692711981.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mheart_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"heart_rate\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mheart_rate_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"predicted_heart_rate\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'dfs' is not defined"
          ]
        }
      ],
      "source": [
        "# ---- Plot an example workout for each variant (same row index) ----\n",
        "index = 0\n",
        "\n",
        "for name, df in dfs.items():\n",
        "    heart_rate = df.iloc[index][\"heart_rate\"]\n",
        "    heart_rate_preds = df.iloc[index][\"predicted_heart_rate\"]\n",
        "    print(f\"Variant: {name} (workout index {index})\")\n",
        "    plot_target_vs_predicted_heartrate(heart_rate, heart_rate_preds)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
